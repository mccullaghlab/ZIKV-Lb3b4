{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bioinformatics Analysis Protocol\n",
    "## Breakdown:\n",
    "### 1) Pre-processing Sequences\n",
    "### 2) MSA analysis\n",
    "### 3) Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Pre-processing Sequences\n",
    "### Goals: Take a large body of sequences (of varying lengths, qualities, sources, etc) and parse this dataset down into sequences of interest.\n",
    "#### Specifically, I am interesting in looking at sequences related to the _helicase domain_ of the _flavivirus_ NS3 protein. \n",
    "Steps:\n",
    "\n",
    "    1) Python script. Remove sequences that are not associated with the desired protein. Chuck a sequence if:\n",
    "        \n",
    "        a) Collect the segment of each sequence that is associated with the helicase domain. To do this, I use a regular expression to find sequence segments of the protein that are known to be strongly conserved. In this case, the regular expression is used to find Motifs I and II (Walker A and B), which are roughly separated by 90 residues in the flavivirus NS3h sequences. _user-defined variables_: regular_expression, average_expression_length, expression_length_deviation, number_residues_pre_expression\n",
    "\n",
    "        b) the sequence is too short to be a sequence of interest; _user-defined variables_: average_sequence_length, sequence_length_deviation\n",
    "        \n",
    "        c) the sequence is poorly resolved (number of 'X', 'B', or 'Z' residues is beyond some cutoff); _user-defined variables_: prob_XBZ\n",
    "            \n",
    "        \n",
    "    2) Hierarchical CD-HIT protocol. Removes identical sequences (every remaining sequence has equal weight); Also used to remove sequence outliers that made it through the python script. \n",
    "        https://github.com/weizhongli/cdhit/wiki/3.-User's-Guide#CDHIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from Bio import SeqIO\n",
    "\n",
    "original_sequence_file = 'all_sources_NS3_sequences.fasta'\n",
    "output_file_name_root = 'cleaned_up_a.fasta'\n",
    "regular_expression = 'G[AS]GK(.+?)DE[AC]H' # regular expression formatting to find motifs I and II, naive about sequences between the given expected strings. \n",
    "average_expression_length = 90\n",
    "expression_length_deviation = 25\n",
    "number_residues_pre_expression = 20\n",
    "average_sequence_length = 450\n",
    "sequence_length_deviation = 30\n",
    "prob_XBZ = 0.25 # cutoff for probability of poorly resolved residues in a sequence\n",
    "\n",
    "sequences = list(SeqIO.parse(original_sequence_file,'fasta'))\n",
    "nSeqs = len(sequences)\n",
    "nSeqs_range = list(range(nSeqs))\n",
    "minimum_sequence_length = average_sequence_length - sequence_length_deviation\n",
    "maximum_sequence_length = average_sequence_length + sequence_length_deviation\n",
    "minimum_expression_length = average_sequence_length - expression_length_deviation\n",
    "maximum_expression_length = average_sequence_length + expression_length_deviation\n",
    "\n",
    "sequence_dictionary = {}\n",
    "with open(output_file_name_root+'.summary.txt','w') as W:\n",
    "    for i in nSeqs_range:     # used to loop over all sequences in original_sequence_file \n",
    "        sequence = str(sequences[i].seq)\n",
    "        last_found = -1\n",
    "        found = []\n",
    "        while True:     # finding all instances of the regular expression within the sequence; \n",
    "            try:\n",
    "                search_string = re.search(regular_expression,sequence[last_found+1:]).group(0)     # finding first instance of regex\n",
    "                zeroth_index_search_string = sequence.find(search_string)     # getting regex zeroth-index in sequence\n",
    "                zeroth_index_sequence = zeroth_index_search_string - number_residues_pre_expression     # actually interested in including residues pre-Motif I, so finding that zeroth-index\n",
    "                if zeroth_index_sequence < 0:     # if regex is found early in sequence, ignore the pre_expression residues zeroth_index, just take the whole sequence\n",
    "                    found.append([zeroth_index_search_string,search_string,len(sequence[:]),sequence[:]])\n",
    "                else:     # grab the sequence associated with pre_expression zeroth-index\n",
    "                    found.append([zeroth_index_search_string,search_string,len(sequence[zeroth_index_sequence:]),sequence[zeroth_index_sequence:]])\n",
    "            except AttributeError:     # if regex isn't found, break from the while loop\n",
    "                break\n",
    "            last_found = sequence.find(search_string,last_found+1)     # update the last_found variable to find more instances of the regex in sequence[i]\n",
    "        \n",
    "        if last_found == -1:     # last_found is never updated if no regex is found, which indicates that important motifs are not present in the sequence.\n",
    "            W.write('%-5d %s Regular expression input not found; removed this sequence from the population of sequences.\\n'%(i,sequences[i].id))\n",
    "            continue\n",
    "        for j in found:     # Analyze every instance of the regex being found within the sequence\n",
    "            if j[2] < minimum_sequence_length:     # compare length of sequence to min sequence length; if too short, remove sequence\n",
    "                W.write('%-5d %s Regular expression input found but the remaining sequence is too short (length = %d) to be the full, desired sequence.\\n'%(i,sequences[i].id,j[2]))\n",
    "                continue\n",
    "            elif len(j[1]) < minimum_expression_length or len(j[1]) > maximum_expression_length:     # compare length of sequence of the regex; if of unexpected length, remove sequence\n",
    "                W.Write('%-5d %s Regular expression input found but length of the search string is unexpected (length = %d) (expected length w/in %d to %d residues)'%(i,sequences[i].id,len(j[1]),minimum_expression_length,maximum_expression_length))\n",
    "                continue\n",
    "            elif j[2] > maximum_sequence_length:     # compare length of sequence to max sequence length; only grab the first avg_sequence_length residues and keep sequence\n",
    "                sequence = j[3][:avg_sequence_length+1]\n",
    "                W.write('%-5d %s Regular expression input found but length of sequence is longer than expected; truncating (post-search_string) segment assuming the sequence should be approximately the average length.'%(i,sequences[i].id))\n",
    "            else:\n",
    "                sequence = j[3]\n",
    "                W.write('%-5d %s Regular expression input found; sequence length is within expected range. ' %(i,sequences[i].id))\n",
    "            \n",
    "            if float(sequence.count('X') + sequence.count('B') + sequence.count('Z'))/len(sequence) <= prob_XBZ:     # test if prob of a poorly resolved AA symbol is greater than prob_XBZ; if <=, then keep sequence \n",
    "                if sequence not in sequence_dictionary:     # test if sequence hasn't already been ID'ed; if it hasn't, create a new dictionary key and value\n",
    "                    sequence_dictionary[sequence] = sequences[i].id\n",
    "                    W.write('This sequence string is the first instance of such a sequence; Adding sequence to sequence dictionary.\\n')\n",
    "                else:     # if it has, add the sequence ID to the end of the dictionary value (which is the sequence ID of previously-found sequences)\n",
    "                    sequence_dictionary[sequence] += '___' + sequences[i].id\n",
    "                    W.write('This sequence string has been seen before; appending sequence info to previous sequence_dictioanry value.\\n')\n",
    "            else:\n",
    "                W.write('This sequence string is poor resolution. Removed from the population of sequences.\\n')\n",
    "\n",
    "with open(output_file_name_root+'.fasta','w') as output_file:     # output the fasta file associated with this pre-processing\n",
    "    for sequence in sequence_dictionary:\n",
    "        output_file.write('>'+sequence_dictionary[sequence]+'\\n'+sequence+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Remove degenerate sequences that were not caught during step 1 as well as removing non-flavivirus NS3h sequences by using the CD-HIT software. See https://github.com/weizhongli/cdhit/wiki/3.-User's-Guide#CDHIT for a user guide.\n",
    "\n",
    "\"CD-HIT clusters proteins into clusters that meet a user-defined similarity threshold, usually a sequence identity. Each cluster has one representative sequence. The input is a protein dataset in fasta format and the output are two files: a fasta file of representative sequences and a text file of list of clusters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering step 1: threshold value = 1.0\n",
    "\n",
    "Removes degenerate (completely identical) sequences within the ensemble. This step is important because it normalizes the weight associated with every unique sequence; we aren't interested in the frequency that a specific sequence of NS3h is observed, just the population of sequences. This might be a source of human bias. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd-hit -i processed_NS3_sequences.fasta -o clustered_1.0.fasta -c 1.0 -n 5 -T 10 -M 16000 -d 0 -g 1 -sc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering step 2: threshold value = 0.4\n",
    "\n",
    "Using a small threshold value separates sequences into families (e.g. hepacivirus, pestiviruses, and flavivirus sequences) as well as removes extreme outliers that aren't clustered into a family."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd-hit -i clustered_1.0.fasta -o clustered_0.4.fasta -c 0.4 -n 2 -T 10 -M 16000 -d 0 -g 1 -sc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CD-HIT software package comes with a perl script that will take the clustering results and prepare a fasta file for each cluster. This file can then be used, for example, in a multisequence alignment analysis for each cluster.\n",
    "\n",
    "How to use: \n",
    "\n",
    "\"perl /home/apps/cdhit/make_multi_seq.pl original_sequence_file clustering_analysis_output_file.clstr new_directory_name num_sequences_in_cluster\"\n",
    "\n",
    "where \n",
    "\n",
    "      original_sequence_file is the input fasta file to the cd-hit analysis\n",
    "\n",
    "      clustering_analysis_output_file.clstr is the cd-hit cluster output file \n",
    "      \n",
    "      new_directory_name is a directory to be created to hold the clusters' fasta files to be created\n",
    "      \n",
    "      num_sequence_in_cluster is an integer value that functions as a minimum number of sequences in a cluster to be analyzed further; this variable is used to remove extreme outliers from the weak clustering threshold"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "perl /home/apps/cdhit/make_multi_seq.pl clustered_1.0.fasta clustered_0.4.fasta.clstr clustered_0.4 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, I am interested in looking at only flaviviral sequences (which is likely associated to cluster 1 and respective file named '1')."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd clustered_0.4/\n",
    "mv 1 flaviviral_sequences.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering step 3: threshold value = 0.8\n",
    "\n",
    "Cluster sequences one last time with a consistency cutoff of 0.8. Use the perl script to output well represented clusters. Combine the cluster fasta files into one to be analyzed in the MSA."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd-hit -i flaviviral_sequences.fasta -o flaviviral_clustered_0.8.fasta -c 0.8 -n 5 -T 10 -M 16000 -d 0 -g 1 -sc 1\n",
    "perl /home/apps/cdhit/make_multi_seq.pl flaviviral_sequences.fasta flaviviral_clustered_0.8.fasta.clstr flaviviral_clustered_0.8 25\n",
    "cd flaviviral_clustered_0.8\n",
    "# combine all output fasta files into one file\n",
    "cat 0 >  combined_sequences.fasta \n",
    "cat 1 >> combined_sequences.fasta\n",
    ".\n",
    ".\n",
    ".\n",
    "cat 6 >> combined_sequences.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Multiple Sequence Alignment (MSA) Analysis\n",
    "https://mafft.cbrc.jp/alignment/software/algorithms/algorithms.html\n",
    "### Goals: Take processed sequences and calculate a high quality MSA. There are numerous methods within the MAFFT software available to do so. :. perform MSA using a subset of these methods and compare results.\n",
    "#### Specifically, I am interested in obtaining the highest quality MSA possible. Unfortunately, I am unfamiliar with the MSA algorithms and scoring functions used within the software, so I will initially use brute force to obtain the desired MSA.\n",
    "Steps:\n",
    "\n",
    "    1) Run MAFFT software on the same sequence file using three different methods. Outputs will all be different. \n",
    "    \n",
    "    2) Use the TRIMAL software to quantitatively decide which MSA result is best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "Progressive alignment with iterative refinement: FFT-NS-i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mafft --retree 2 --maxiterate 1000 --thread 10 combined_sequences.fasta > combined_mafft_fft_ns_i.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local alignment using iterative refinement methods using WSP and consistency scores: L-INS-i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mafft --localpair --maxiterate 1000 --thread 10 combined_sequences.fasta > combined_mafft_l_ins_i.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global alignment using iterative refinement methods using WSP and consistency scores: G-INS-i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mafft --globalpair --maxiterate 1000 --thread 10 combined_sequences.fasta > combined_mafft_g_ins_i.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Trimal software allows for quantitative comparison of MSA alignments. To do so, I need to create a txt file that has the locations of each msa file on new lines (compareset_input_file.txt)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trimal -compareset compareset_input_file.txt -out trimal_compareset_gt_0.8 -gt 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, trimal compares the mafft methods to identify the highest quality alignment based on some (unknown to me) scoring function and outputs the respective MSA file with columns removed that have gaps in more than 20% of all sequences. The choice of 20% is a potential source of bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis \n",
    "### Goals: Mine the MSA results to obtain novel insight into the biophysics of the NS3 helicase domain. Couple the insights gained from MD simulations with the bioinformatics results stemming from the MSA. \n",
    "#### Specifically, I am interested in using the MSA results to guide my analysis of MD simulations (e.g. choice of collective variables that are strongly conserved across all flaviviruses, allowing for their use in numerous bodies of simulations). \n",
    "Steps:\n",
    "\n",
    "    1) Use a python script to calculate basic results from the MSA. Specifically, the consensus sequence, position frequency matrix, and mutual information. Interpret/couple these results with the MD simulations.\n",
    "    \n",
    "    2) Use the weblogo (http://weblogo.threeplusone.com/) web-based application to create a figure presenting the MSA results. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-analysis python script usage:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python3 post_alignment_analyses.py trimal_compareset_gt_0.8.fasta flavivirus_NS3h_msa_results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
